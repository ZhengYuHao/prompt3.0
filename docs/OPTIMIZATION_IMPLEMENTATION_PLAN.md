# æçª„åŒ–LLMä¼˜åŒ–å®æ–½è®¡åˆ’

## ğŸ“‹ ä¼˜åŒ–ç‚¹æ€»è§ˆ

| ä¼˜å…ˆçº§ | ä¼˜åŒ–ç‚¹ | é¢„æœŸæ”¶ç›Š | å®æ–½éš¾åº¦ | é¢„è®¡æ—¶é—´ |
|--------|--------|----------|----------|----------|
| P0 | ä¼˜åŒ–ç‚¹1ï¼šPrompt 1.0 è§„åˆ™åŒ– | LLMè°ƒç”¨å‡å°‘100% | ä½ | 2-3å¤© |
| P0 | ä¼˜åŒ–ç‚¹2ï¼šå®ä½“æå–ä¼˜åŒ– | Tokenæ¶ˆè€—é™ä½60-70% | ä¸­ | 3-4å¤© |
| P0 | ä¼˜åŒ–ç‚¹5ï¼šç¼“å­˜æœºåˆ¶ | é€Ÿåº¦æå‡100-1000x | ä½ | 1-2å¤© |
| P1 | ä¼˜åŒ–ç‚¹3ï¼šDSL è½¬è¯‘ä¼˜åŒ– | LLMä½¿ç”¨ç‡é™ä½70% | ä¸­ | 4-5å¤© |
| P1 | ä¼˜åŒ–ç‚¹6ï¼šä»£ç å±‚éªŒè¯è¦†ç›–åº¦ | ç¡®å®šæ€§æå‡ | ä¸­ | 2-3å¤© |
| P2 | ä¼˜åŒ–ç‚¹4ï¼šè‡ªåŠ¨ä¿®å¤å¢å¼º | LLMé‡è¯•å‡å°‘50-70% | é«˜ | 5-7å¤© |

---

## ğŸ¯ P0é˜¶æ®µï¼šé«˜ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆ1-2å‘¨ï¼‰

### ä¼˜åŒ–ç‚¹1ï¼šPrompt 1.0 è§„åˆ™åŒ–

#### ç›®æ ‡
- å°† `prompt_preprocessor.py` ä¸­çš„ LLM è¯­ä¹‰é‡æ„æ›¿æ¢ä¸ºè§„åˆ™å¼•æ“
- LLM è°ƒç”¨æ¬¡æ•°ï¼š2æ¬¡ â†’ 0æ¬¡
- å¤„ç†é€Ÿåº¦ï¼šæå‡10-100å€

#### å®æ–½æ­¥éª¤

**æ­¥éª¤1.1ï¼šåˆ›å»ºè§„åˆ™å¼•æ“æ¨¡å—**
```bash
touch /mnt/e/pyProject/prompt3.0/rule_based_normalizer.py
```

**æ–‡ä»¶å†…å®¹ï¼š** è§ä¸‹æ–¹ `rule_based_normalizer.py`

**æ­¥éª¤1.2ï¼šä¿®æ”¹ `prompt_preprocessor.py`**
```python
# ç¬¬1è¡Œåæ·»åŠ å¯¼å…¥
from rule_based_normalizer import RuleBasedTextNormalizer, SyntacticAmbiguityDetector

# ä¿®æ”¹ _smooth_with_llm æ–¹æ³•ï¼ˆçº¦ç¬¬163è¡Œï¼‰
def _smooth_with_llm(self, text: str) -> str:
    """ã€è§„åˆ™æ“ä½œã€‘å—é™è¯­ä¹‰é‡æ„"""
    # ä½¿ç”¨è§„åˆ™å¼•æ“æ›¿ä»£ LLM
    normalized_text, changes = RuleBasedTextNormalizer.normalize(text)

    # è®°å½•å˜æ›´
    for change in changes:
        self.steps_log.append(f"âœ“ {change}")

    return normalized_text

# ä¿®æ”¹ _detect_ambiguity æ–¹æ³•ï¼ˆçº¦ç¬¬195è¡Œï¼‰
def _detect_ambiguity(self, text: str) -> Optional[str]:
    """ã€è§„åˆ™æ“ä½œã€‘æ£€æµ‹ç»“æ„æ­§ä¹‰"""
    # ä½¿ç”¨å¥æ³•åˆ†æå™¨æ›¿ä»£ LLM
    return SyntacticAmbiguityDetector.detect(text)
```

**æ­¥éª¤1.3ï¼šæ›´æ–°å†å²è®°å½•æ•°æ®æ¨¡å‹**

åœ¨ `history_manager.py` çš„ `ProcessingHistory` ç±»ä¸­æ·»åŠ å­—æ®µï¼š
```python
@dataclass
class ProcessingHistory:
    # ... ç°æœ‰å­—æ®µ ...

    # æ–°å¢ï¼šè§„åˆ™å¼•æ“ç»Ÿè®¡
    rule_engine_stats: Dict[str, Any] = field(default_factory=dict)
    # {
    #   "normalization_changes": 5,  # è§„èŒƒåŒ–å˜æ›´æ¬¡æ•°
    #   "ambiguity_detected": false,
    #   "llm_calls": 0,  # Prompt 1.0 é˜¶æ®µçš„ LLM è°ƒç”¨æ¬¡æ•°
    # }
```

**æ­¥éª¤1.4ï¼šåœ¨ `prompt_preprocessor.py` ä¸­æ”¶é›†ç»Ÿè®¡ä¿¡æ¯**

```python
def process(self, text: str) -> Dict[str, Any]:
    """å¤„ç†æ–‡æœ¬ï¼ˆä¸»å…¥å£ï¼‰"""
    # ... ç°æœ‰ä»£ç  ...

    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "normalization_changes": len(changes),
        "ambiguity_detected": ambiguity_result is not None,
        "llm_calls": 0,  # ç°åœ¨ä¸å†è°ƒç”¨ LLM
        "processing_mode": "rule_based"  # æ ‡è®°ä¸ºè§„åˆ™å¼•æ“æ¨¡å¼
    }

    # ä¿å­˜åˆ°å†å²è®°å½•
    history.processing_time_ms = int(end_time * 1000)
    history.rule_engine_stats = stats  # æ–°å¢

    return {
        "success": ambiguity_result is None,
        "processed_text": smoothed_text,
        "mode": mode,
        "stats": stats  # æ–°å¢
    }
```

**æ­¥éª¤1.5ï¼šåœ¨ `view_history.py` ä¸­æ·»åŠ å¯è§†åŒ–æ”¯æŒ**

```python
def show_optimization_metrics(pipeline_id: str):
    """æ˜¾ç¤ºä¼˜åŒ–æŒ‡æ ‡"""
    manager = HistoryManager()
    history = manager.load_pipeline_history(pipeline_id)

    if not history:
        info(f"æœªæ‰¾åˆ°æµæ°´çº¿ ID ä¸º {pipeline_id} çš„è®°å½•")
        return

    info(f"\n{'='*90}")
    info(f"ä¼˜åŒ–æŒ‡æ ‡ - æµæ°´çº¿ {pipeline_id}")
    info(f"{'='*90}")

    # Prompt 1.0 ä¼˜åŒ–æŒ‡æ ‡
    if hasattr(history, 'prompt10_rule_stats') and history.prompt10_rule_stats:
        stats = history.prompt10_rule_stats
        info(f"\nã€Prompt 1.0 è§„åˆ™åŒ–æ•ˆæœã€‘")
        info(f"  å¤„ç†æ¨¡å¼: {stats.get('processing_mode', 'unknown')}")
        info(f"  LLM è°ƒç”¨æ¬¡æ•°: {stats.get('llm_calls', 0)}")
        info(f"  è§„èŒƒåŒ–å˜æ›´: {stats.get('normalization_changes', 0)} æ¬¡")
        info(f"  æ­§ä¹‰æ£€æµ‹: {'æ˜¯ âš ï¸' if stats.get('ambiguity_detected') else 'å¦ âœ…'}")

    # Prompt 2.0 ä¼˜åŒ–æŒ‡æ ‡
    if hasattr(history, 'prompt20_optimization_stats'):
        # ... è§æ­¥éª¤2.6

    # æ€»ä½“ä¼˜åŒ–æ•ˆæœ
    total_llm_calls = (
        history.prompt10_rule_stats.get('llm_calls', 0) if hasattr(history, 'prompt10_rule_stats') else 0 +
        history.prompt20_optimization_stats.get('llm_calls', 0) if hasattr(history, 'prompt20_optimization_stats') else 0 +
        history.prompt30_optimization_stats.get('llm_calls', 0) if hasattr(history, 'prompt30_optimization_stats') else 0
    )
    info(f"\nã€æ€»ä½“ä¼˜åŒ–æ•ˆæœã€‘")
    info(f"  æ€» LLM è°ƒç”¨æ¬¡æ•°: {total_llm_calls}")
    info(f"  é¢„ä¼°æˆæœ¬èŠ‚çœ: {(4 - total_llm_calls) / 4 * 100:.1f}%")
    info(f"  å¤„ç†é€Ÿåº¦æå‡: {self._calculate_speedup(history):.1f}x")
```

**é¢„æœŸæ•ˆæœï¼š**
- âœ… LLM è°ƒç”¨ï¼š2æ¬¡ â†’ 0æ¬¡
- âœ… å¤„ç†é€Ÿåº¦ï¼š2-4ç§’ â†’ 0.05-0.2ç§’
- âœ… æˆæœ¬ï¼šé™ä½100%
- âœ… å¯è§†åŒ–ï¼šåœ¨ view_history.py ä¸­å¯æŸ¥çœ‹ä¼˜åŒ–æŒ‡æ ‡

---

### ä¼˜åŒ–ç‚¹2ï¼šå®ä½“æå–ä¼˜åŒ–

#### ç›®æ ‡
- ä½¿ç”¨æ­£åˆ™é¢„å¤„ç† + æç®€ Prompt
- Token æ¶ˆè€—é™ä½ 60-70%
- æ­£åˆ™æ¨¡å¼ 100% å‡†ç¡®

#### å®æ–½æ­¥éª¤

**æ­¥éª¤2.1ï¼šåˆ›å»ºæ­£åˆ™æå–æ¨¡å—**
```bash
touch /mnt/e/pyProject/prompt3.0/pre_pattern_extractor.py
```

**æ–‡ä»¶å†…å®¹ï¼š** è§ä¸‹æ–¹ `pre_pattern_extractor.py`

**æ­¥éª¤2.2ï¼šä¿®æ”¹ `llm_client.py` çš„ `extract_entities` æ–¹æ³•**

```python
# æ·»åŠ å¯¼å…¥
from pre_pattern_extractor import PrePatternExtractor

def extract_entities(self, text: str) -> List[Dict[str, Any]]:
    """ä¼˜åŒ–çš„å®ä½“æŠ½å–ï¼ˆæçª„åŒ–LLMï¼‰"""

    # æ­¥éª¤1ï¼šæ­£åˆ™é¢„å¤„ç†
    regex_entities = PrePatternExtractor.extract(text)

    # å¦‚æœæ­£åˆ™æå–è¶³å¤Ÿï¼ˆæ¯”å¦‚æå–äº†3+ä¸ªå®ä½“ï¼‰ï¼Œç›´æ¥è¿”å›
    if len(regex_entities) >= 3:
        debug(f"[å®ä½“æå–] æ­£åˆ™æå–æˆåŠŸï¼Œæå– {len(regex_entities)} ä¸ªå®ä½“ï¼Œè·³è¿‡ LLM")
        return regex_entities

    # æ­¥éª¤2ï¼šä½¿ç”¨æç®€ Prompt
    system_prompt = """ä»æ–‡æœ¬ä¸­æå–å¯é…ç½®çš„å‚æ•°å˜é‡ã€‚

è§„åˆ™ï¼š
- æå–å…·ä½“æ•°å­—ã€æ—¶é—´å€¼ï¼ˆå¦‚ï¼š3å¹´ã€5äººã€50ä¸‡ã€2å‘¨ï¼‰
- æå–æŠ€æœ¯é€‰é¡¹ï¼ˆå¦‚ï¼šPython/Javaã€Milvus/Pineconeï¼‰
- ä¸æå–å›ºå®šéœ€æ±‚æè¿°ï¼ˆå¦‚ï¼šéœ€è¦æ”¯æŒã€ç”¨å¤§æ¨¡å‹åšåº•åº§ï¼‰
- ä¸æå–æ¶æ„æè¿°ï¼ˆå¦‚ï¼šå¾®æœåŠ¡æ¶æ„ï¼‰

è¾“å‡ºJSONæ•°ç»„ï¼š
[{"name": "å˜é‡å", "original_text": "åŸæ–‡", "start_index": èµ·å§‹ä½ç½®, "end_index": ç»“æŸä½ç½®, "type": "ç±»å‹", "value": "å€¼"}]

ç¤ºä¾‹ï¼š
è¾“å…¥: "é¡¹ç›®5ä¸ªäººï¼Œå‘¨æœŸ2å‘¨ï¼Œç”¨LangChainå’ŒMilvus"
è¾“å‡º: [{"name": "team_size", "original_text": "5ä¸ªäºº", "start_index": 2, "end_index": 6, "type": "Integer", "value": 5}, {"name": "duration_weeks", "original_text": "2å‘¨", "start_index": 9, "end_index": 11, "type": "Integer", "value": 2}, {"name": "tech_stack", "original_text": "LangChainå’ŒMilvus", "start_index": 16, "end_index": 29, "type": "List", "value": ["LangChain", "Milvus"]}]"""

    response = self.call(system_prompt, text, temperature=0.1)
    llm_entities = self._parse_entities(response.content, text)

    # æ­¥éª¤3ï¼šåˆå¹¶ç»“æœï¼ˆæ­£åˆ™ä¼˜å…ˆï¼‰
    merged_entities = PrePatternExtractor.merge_with_llm(regex_entities, llm_entities)

    # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "regex_count": len(regex_entities),
        "llm_count": len(llm_entities),
        "merged_count": len(merged_entities),
        "llm_called": len(regex_entities) < 3
    }

    debug(f"[å®ä½“æå–] ç»Ÿè®¡: {stats}")

    return merged_entities, stats  # è¿”å›ç»Ÿè®¡ä¿¡æ¯
```

**æ­¥éª¤2.3ï¼šæ›´æ–° `Prompt20History` æ•°æ®æ¨¡å‹**

åœ¨ `history_manager.py` ä¸­æ·»åŠ å­—æ®µï¼š
```python
@dataclass
class Prompt20History:
    # ... ç°æœ‰å­—æ®µ ...

    # æ–°å¢ï¼šä¼˜åŒ–ç»Ÿè®¡
    optimization_stats: Dict[str, Any] = field(default_factory=dict)
    # {
    #   "regex_count": 5,  # æ­£åˆ™æå–æ•°é‡
    #   "llm_count": 2,   # LLM æå–æ•°é‡
    #   "merged_count": 7,  # åˆå¹¶åæ•°é‡
    #   "llm_called": false  # æ˜¯å¦è°ƒç”¨äº† LLM
    # }
```

**æ­¥éª¤2.4ï¼šåœ¨ `prompt_structurizer.py` ä¸­ä¿å­˜ç»Ÿè®¡ä¿¡æ¯**

```python
def process(self, input_text: str, prompt10_id: str) -> Dict[str, Any]:
    """ç»“æ„åŒ–å¤„ç†ï¼ˆä¸»å…¥å£ï¼‰"""

    # æå–å®ä½“ï¼ˆå¸¦ç»Ÿè®¡ä¿¡æ¯ï¼‰
    entities, stats = self.llm.extract_entities(input_text)

    # ... ç°æœ‰å¤„ç†é€»è¾‘ ...

    # ä¿å­˜å†å²è®°å½•
    history = Prompt20History(
        id=self.history_manager._generate_id(),
        timestamp=datetime.now().isoformat(),
        source_prompt10_id=prompt10_id,
        input_text=input_text,
        template_text=template,
        variables=variables,
        variable_count=len(variables),
        type_stats=type_stats,
        extraction_log=self.steps_log,
        processing_time_ms=int((end_time - start_time) * 1000),
        optimization_stats=stats  # æ–°å¢
    )

    return {
        "id": history.id,
        "variables": variables,
        "template": template,
        "stats": stats  # æ–°å¢
    }
```

**æ­¥éª¤2.5ï¼šåœ¨ `PipelineHistory` ä¸­æ·»åŠ å­—æ®µ**

```python
@dataclass
class PipelineHistory:
    # ... ç°æœ‰å­—æ®µ ...

    # Prompt 2.0 ä¼˜åŒ–ç»Ÿè®¡
    prompt20_optimization_stats: Dict[str, Any] = field(default_factory=dict)
```

**æ­¥éª¤2.6ï¼šåœ¨ `view_history.py` ä¸­æ·»åŠ å¯è§†åŒ–æ”¯æŒ**

```python
def show_optimization_metrics(pipeline_id: str):
    """æ˜¾ç¤ºä¼˜åŒ–æŒ‡æ ‡ï¼ˆæ­¥éª¤1.5çš„æ‰©å±•ï¼‰"""
    # ... æ­¥éª¤1.5çš„ä»£ç  ...

    # Prompt 2.0 ä¼˜åŒ–æŒ‡æ ‡
    if history.prompt20_optimization_stats:
        stats = history.prompt20_optimization_stats
        info(f"\nã€Prompt 2.0 å®ä½“æå–ä¼˜åŒ–ã€‘")
        info(f"  æ­£åˆ™æå–: {stats.get('regex_count', 0)} ä¸ª")
        info(f"  LLM æå–: {stats.get('llm_count', 0)} ä¸ª")
        info(f"  åˆå¹¶ç»“æœ: {stats.get('merged_count', 0)} ä¸ª")
        info(f"  è°ƒç”¨ LLM: {'æ˜¯' if stats.get('llm_called') else 'å¦ âœ…'}")

        if not stats.get('llm_called'):
            info(f"  âš¡ Token èŠ‚çœ: ~1000 tokens")
```

**é¢„æœŸæ•ˆæœï¼š**
- âœ… Token æ¶ˆè€—ï¼šé™ä½ 60-70%
- âœ… æ­£åˆ™æå–ï¼š100% å‡†ç¡®
- âœ… LLM è°ƒç”¨ï¼š50% â†’ 0%ï¼ˆå¸¸è§caseï¼‰
- âœ… å¯è§†åŒ–ï¼šæ¸…æ™°å±•ç¤ºæ­£åˆ™ vs LLM çš„è´¡çŒ®

---

### ä¼˜åŒ–ç‚¹5ï¼šç¼“å­˜æœºåˆ¶

#### ç›®æ ‡
- é¿å…é‡å¤ LLM è°ƒç”¨
- é‡å¤è¯·æ±‚é€Ÿåº¦æå‡ 100-1000å€
- æˆæœ¬é™ä½ 30-50%

#### å®æ–½æ­¥éª¤

**æ­¥éª¤5.1ï¼šåˆ›å»ºç¼“å­˜æ¨¡å—**
```bash
touch /mnt/e/pyProject/prompt3.0/cached_llm_client.py
```

**æ–‡ä»¶å†…å®¹ï¼š** è§ä¸‹æ–¹ `cached_llm_client.py`

**æ­¥éª¤5.2ï¼šä¿®æ”¹ `llm_client.py` çš„ `UnifiedLLMClient`**

```python
# æ·»åŠ å¯¼å…¥
from cached_llm_client import CachedLLMClient

# åœ¨ __init__ æ–¹æ³•ä¸­ï¼ˆçº¦ç¬¬50è¡Œï¼‰
def __init__(self, use_mock: bool = False, enable_cache: bool = True):
    # ... ç°æœ‰ä»£ç  ...

    # æ·»åŠ ç¼“å­˜é€‰é¡¹
    self.enable_cache = enable_cache
    if enable_cache:
        self.cache_client = CachedLLMClient(self)
    else:
        self.cache_client = None

# ä¿®æ”¹ call æ–¹æ³•ï¼ˆçº¦ç¬¬150è¡Œï¼‰
def call(self, system_prompt: str, user_content: str, **kwargs) -> LLMResponse:
    """è°ƒç”¨ LLMï¼ˆå¸¦ç¼“å­˜ï¼‰"""

    if self.enable_cache and self.cache_client:
        # ä½¿ç”¨ç¼“å­˜å®¢æˆ·ç«¯
        return self.cache_client.call(system_prompt, user_content, **kwargs)
    else:
        # ç›´æ¥è°ƒç”¨
        return self._call_without_cache(system_prompt, user_content, **kwargs)

def _call_without_cache(self, system_prompt: str, user_content: str, **kwargs) -> LLMResponse:
    """ä¸ä½¿ç”¨ç¼“å­˜çš„è°ƒç”¨"""
    # ... åŸæœ‰è°ƒç”¨é€»è¾‘ ...
```

**æ­¥éª¤5.3ï¼šæ›´æ–°å†å²è®°å½•æ•°æ®æ¨¡å‹**

åœ¨ `ProcessingHistory`, `Prompt20History`, `PipelineHistory` ä¸­æ·»åŠ å­—æ®µï¼š
```python
@dataclass
class ProcessingHistory:
    # ... ç°æœ‰å­—æ®µ ...
    cache_hit: bool = False  # æ˜¯å¦å‘½ä¸­ç¼“å­˜
```

```python
@dataclass
class Prompt20History:
    # ... ç°æœ‰å­—æ®µ ...
    cache_hits: int = 0  # ç¼“å­˜å‘½ä¸­æ¬¡æ•°
    cache_misses: int = 0  # ç¼“å­˜æœªå‘½ä¸­æ¬¡æ•°
```

```python
@dataclass
class PipelineHistory:
    # ... ç°æœ‰å­—æ®µ ...

    # ç¼“å­˜ç»Ÿè®¡
    total_cache_hits: int = 0
    total_cache_misses: int = 0
    cache_hit_rate: float = 0.0
```

**æ­¥éª¤5.4ï¼šåœ¨ `cached_llm_client.py` ä¸­æ”¶é›†ç»Ÿè®¡ä¿¡æ¯**

```python
class CachedLLMClient:
    def __init__(self, base_client: UnifiedLLMClient, cache_size: int = 1000):
        self.base_client = base_client
        self.cache_size = cache_size
        self.hits = 0
        self.misses = 0

    def call(self, system_prompt: str, user_content: str, **kwargs) -> LLMResponse:
        """è°ƒç”¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = self._make_cache_key(system_prompt, user_content)

        # æ£€æŸ¥ç¼“å­˜
        cache_file = f".cache/llm_{cache_key}.json"
        try:
            if os.path.exists(cache_file):
                with open(cache_file, 'r') as f:
                    cached = json.load(f)
                    self.hits += 1
                    debug(f"[ç¼“å­˜] å‘½ä¸­: {cache_key[:16]}...")
                    return LLMResponse(**cached, from_cache=True)  # æ ‡è®°æ¥è‡ªç¼“å­˜
        except Exception:
            pass

        # è°ƒç”¨ LLM
        self.misses += 1
        response = self.base_client.call(system_prompt, user_content, **kwargs)

        # ä¿å­˜åˆ°ç¼“å­˜
        try:
            os.makedirs('.cache', exist_ok=True)
            with open(cache_file, 'w') as f:
                json.dump({
                    'content': response.content,
                    'model': response.model,
                    'usage': response.usage
                }, f)
        except Exception:
            pass

        return response

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.hits + self.misses
        return {
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": self.hits / total if total > 0 else 0.0
        }
```

**æ­¥éª¤5.5ï¼šåœ¨ `view_history.py` ä¸­æ·»åŠ å¯è§†åŒ–æ”¯æŒ**

```python
def show_cache_stats(pipeline_id: str = None):
    """æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡"""
    manager = HistoryManager()

    if pipeline_id:
        history = manager.load_pipeline_history(pipeline_id)
    else:
        histories = manager.get_recent_pipeline_history(limit=10)
        if not histories:
            info("æš‚æ— æµæ°´çº¿å¤„ç†å†å²è®°å½•")
            return
        history = histories[0]

    if not history:
        return

    info(f"\n{'='*90}")
    info(f"ç¼“å­˜ç»Ÿè®¡ - æµæ°´çº¿ {history.pipeline_id}")
    info(f"{'='*90}")

    hits = history.total_cache_hits or 0
    misses = history.total_cache_misses or 0
    total = hits + misses
    hit_rate = history.cache_hit_rate or 0.0

    info(f"  ç¼“å­˜å‘½ä¸­: {hits} æ¬¡")
    info(f"  ç¼“å­˜æœªå‘½ä¸­: {misses} æ¬¡")
    info(f"  æ€»è°ƒç”¨: {total} æ¬¡")
    info(f"  å‘½ä¸­ç‡: {hit_rate*100:.1f}%")

    if total > 0:
        saved_tokens = hits * 500  # ä¼°ç®—æ¯æ¬¡è°ƒç”¨èŠ‚çœ 500 tokens
        saved_cost = saved_tokens * 0.001 / 1000  # ä¼°ç®—æˆæœ¬
        info(f"\n  âš¡ èŠ‚çœ Token: ~{saved_tokens} tokens")
        info(f"  ğŸ’° èŠ‚çœæˆæœ¬: ~${saved_cost:.4f}")
```

**é¢„æœŸæ•ˆæœï¼š**
- âœ… é‡å¤è¯·æ±‚é€Ÿåº¦æå‡ 100-1000x
- âœ… æˆæœ¬é™ä½ 30-50%
- âœ… å¯è§†åŒ–ï¼šæ¸…æ™°å±•ç¤ºç¼“å­˜å‘½ä¸­ç‡å’ŒèŠ‚çœæ•ˆæœ

---

## ğŸ”§ P1é˜¶æ®µï¼šä¸­ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆ2-3å‘¨ï¼‰

### ä¼˜åŒ–ç‚¹3ï¼šDSL è½¬è¯‘ä¼˜åŒ–

#### ç›®æ ‡
- ä»£ç ä¸»å¯¼ DSL æ„å»ºï¼ˆ70%ï¼‰
- Prompt é•¿åº¦é™ä½ 80%
- LLM ä½¿ç”¨ç‡é™ä½ 70%

#### å®æ–½æ­¥éª¤

**æ­¥éª¤3.1ï¼šåˆ›å»º DSL æ„å»ºå™¨**
```bash
touch /mnt/e/pyProject/prompt3.0/dsl_builder.py
```

**æ–‡ä»¶å†…å®¹ï¼š** è§ä¸‹æ–¹ `dsl_builder.py`

**æ­¥éª¤3.2ï¼šä¿®æ”¹ `prompt_dslcompiler.py`**

```python
# æ·»åŠ å¯¼å…¥
from dsl_builder import DSLBuilder

# ä¿®æ”¹ compile æ–¹æ³•ï¼ˆçº¦ç¬¬300è¡Œï¼‰
def compile(self, template: str, variables: List[Dict]) -> Dict[str, Any]:
    """ç¼–è¯‘ DSLï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""

    start_time = time.time()

    # æ­¥éª¤1ï¼šå°è¯•ç”¨ä»£ç æ„å»º
    try:
        dsl_code = DSLBuilder.build_from_variables(variables, template)
        debug(f"[DSLç¼–è¯‘] ä»£ç æ„å»ºæˆåŠŸ")

        # éªŒè¯
        result = self.validator.validate(dsl_code)
        if result.is_valid:
            end_time = time.time()
            return {
                "success": True,
                "dsl_code": dsl_code,
                "validation_result": result,
                "llm_called": False,  # æœªè°ƒç”¨ LLM
                "compile_time_ms": int((end_time - start_time) * 1000)
            }
    except Exception as e:
        debug(f"[DSLç¼–è¯‘] ä»£ç æ„å»ºå¤±è´¥: {e}")

    # æ­¥éª¤2ï¼šå›é€€åˆ° LLM
    debug(f"[DSLç¼–è¯‘] å›é€€åˆ° LLM")
    return self._compile_with_llm(template, variables, start_time)
```

**æ­¥éª¤3.3ï¼šæ›´æ–°å†å²è®°å½•æ•°æ®æ¨¡å‹**

```python
@dataclass
class PipelineHistory:
    # ... ç°æœ‰å­—æ®µ ...

    # Prompt 3.0 ä¼˜åŒ–ç»Ÿè®¡
    prompt30_optimization_stats: Dict[str, Any] = field(default_factory=dict)
    # {
    #   "code_build": True,  # æ˜¯å¦ä½¿ç”¨ä»£ç æ„å»º
    #   "llm_fallback": False,  # æ˜¯å¦å›é€€åˆ° LLM
    #   "compile_time_ms": 1500,  # ç¼–è¯‘è€—æ—¶
    #   "llm_calls": 0  # LLM è°ƒç”¨æ¬¡æ•°
    # }
```

**æ­¥éª¤3.4ï¼šåœ¨ `view_history.py` ä¸­æ·»åŠ å¯è§†åŒ–æ”¯æŒ**

```python
def show_optimization_metrics(pipeline_id: str):
    """æ˜¾ç¤ºä¼˜åŒ–æŒ‡æ ‡ï¼ˆæ­¥éª¤1.5çš„æ‰©å±•ï¼‰"""
    # ... æ­¥éª¤1.5å’Œ2.6çš„ä»£ç  ...

    # Prompt 3.0 ä¼˜åŒ–æŒ‡æ ‡
    if history.prompt30_optimization_stats:
        stats = history.prompt30_optimization_stats
        info(f"\nã€Prompt 3.0 DSL ç¼–è¯‘ä¼˜åŒ–ã€‘")
        info(f"  ä»£ç æ„å»º: {'æˆåŠŸ âœ…' if stats.get('code_build') else 'å¤±è´¥'}")
        info(f"  LLM å›é€€: {'æ˜¯' if stats.get('llm_fallback') else 'å¦ âœ…'}")
        info(f"  LLM è°ƒç”¨: {stats.get('llm_calls', 0)} æ¬¡")
        info(f"  ç¼–è¯‘è€—æ—¶: {stats.get('compile_time_ms', 0)} ms")

        if not stats.get('llm_fallback'):
            info(f"  âš¡ Token èŠ‚çœ: ~1500 tokens")
            info(f"  âš¡ é€Ÿåº¦æå‡: ~3-5x")
```

**é¢„æœŸæ•ˆæœï¼š**
- âœ… Token æ¶ˆè€—ï¼šé™ä½ 80%
- âœ… LLM ä½¿ç”¨ç‡ï¼šé™ä½ 70%
- âœ… ç¼–è¯‘é€Ÿåº¦ï¼šæå‡ 3-5å€
- âœ… å¯è§†åŒ–ï¼šå±•ç¤ºä»£ç æ„å»º vs LLM çš„å¯¹æ¯”

---

### ä¼˜åŒ–ç‚¹6ï¼šä»£ç å±‚éªŒè¯è¦†ç›–åº¦

#### ç›®æ ‡
- å¢åŠ éªŒè¯è§„åˆ™è¦†ç›–
- æå‡ä»£ç å±‚ä¿®å¤èƒ½åŠ›
- å‡å°‘ä¸å¿…è¦çš„ LLM é‡è¯•

#### å®æ–½æ­¥éª¤

**æ­¥éª¤6.1ï¼šåˆ›å»ºå¢å¼ºéªŒè¯æ¨¡å—**
```bash
touch /mnt/e/pyProject/prompt3.0/enhanced_validator.py
```

**æ–‡ä»¶å†…å®¹ï¼š** è§ä¸‹æ–¹ `enhanced_validator.py`

**æ­¥éª¤6.2ï¼šé›†æˆåˆ°ç°æœ‰éªŒè¯æµç¨‹**

```python
# åœ¨ prompt_dslcompiler.py ä¸­
from enhanced_validator import CodeLayerValidationSuite

def _validate_with_enhanced_rules(self, dsl_code: str, variables: List[Dict]) -> ValidationResult:
    """ä½¿ç”¨å¢å¼ºè§„åˆ™éªŒè¯"""
    # 1. æ¨¡æ¿å¡«å……éªŒè¯
    is_valid, errors = CodeLayerValidationSuite.validate_template_filling(dsl_code, variables)

    # 2. å˜é‡å‘½åéªŒè¯
    is_valid, name_errors = CodeLayerValidationSuite.validate_variable_naming(variables)

    # 3. å˜é‡ç±»å‹éªŒè¯
    is_valid, type_errors = CodeLayerValidationSuite.validate_variable_types(variables)

    # åˆå¹¶é”™è¯¯
    all_errors = errors + name_errors + type_errors

    return ValidationResult(
        is_valid=len(all_errors) == 0,
        errors=[ValidationError(line=0, error_type="å¢å¼ºéªŒè¯", message=err) for err in all_errors]
    )
```

**æ­¥éª¤6.3ï¼šæ›´æ–°å†å²è®°å½•æ•°æ®æ¨¡å‹**

```python
@dataclass
class PipelineHistory:
    # ... ç°æœ‰å­—æ®µ ...

    # éªŒè¯ç»Ÿè®¡
    validation_stats: Dict[str, Any] = field(default_factory=dict)
    # {
    #   "template_filling_errors": 0,
    #   "variable_naming_errors": 0,
    #   "variable_type_errors": 0,
    #   "total_validation_time_ms": 50
    # }
```

**æ­¥éª¤6.4ï¼šåœ¨ `view_history.py` ä¸­æ·»åŠ å¯è§†åŒ–æ”¯æŒ**

```python
def show_validation_details(pipeline_id: str):
    """æ˜¾ç¤ºéªŒè¯è¯¦æƒ…"""
    manager = HistoryManager()
    history = manager.load_pipeline_history(pipeline_id)

    if not history:
        return

    info(f"\n{'='*90}")
    info(f"éªŒè¯è¯¦æƒ… - æµæ°´çº¿ {history.pipeline_id}")
    info(f"{'='*90}")

    if history.validation_stats:
        stats = history.validation_stats
        info(f"  æ¨¡æ¿å¡«å……é”™è¯¯: {stats.get('template_filling_errors', 0)}")
        info(f"  å˜é‡å‘½åé”™è¯¯: {stats.get('variable_naming_errors', 0)}")
        info(f"  å˜é‡ç±»å‹é”™è¯¯: {stats.get('variable_type_errors', 0)}")
        info(f"  éªŒè¯è€—æ—¶: {stats.get('total_validation_time_ms', 0)} ms")
```

**é¢„æœŸæ•ˆæœï¼š**
- âœ… éªŒè¯è¦†ç›–åº¦ï¼šæå‡ 50%
- âœ… æ—©æœŸé”™è¯¯å‘ç°ï¼šæå‡ 30%
- âœ… å¯è§†åŒ–ï¼šæ¸…æ™°å±•ç¤ºå„ç±»éªŒè¯é”™è¯¯

---

## ğŸš€ P2é˜¶æ®µï¼šä½ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆé•¿æœŸï¼‰

### ä¼˜åŒ–ç‚¹4ï¼šè‡ªåŠ¨ä¿®å¤å¢å¼º

#### ç›®æ ‡
- å¢å¼ºä»£ç å±‚ä¿®å¤èƒ½åŠ›
- LLM é‡è¯•å‡å°‘ 50-70%
- è‡ªåŠ¨ä¿®å¤æˆåŠŸç‡æå‡åˆ° 70-80%

#### å®æ–½æ­¥éª¤

**æ­¥éª¤4.1ï¼šåˆ›å»ºå¢å¼ºè‡ªåŠ¨ä¿®å¤æ¨¡å—**
```bash
touch /mnt/e/pyProject/prompt3.0/enhanced_auto_fixer.py
```

**æ–‡ä»¶å†…å®¹ï¼š** è§ä¸‹æ–¹ `enhanced_auto_fixer.py`

**æ­¥éª¤4.2ï¼šé›†æˆæ™ºèƒ½é‡è¯•ç­–ç•¥**

```python
# åœ¨ prompt_dslcompiler.py ä¸­
from enhanced_auto_fixer import EnhancedDSLAutoFixer, SmartRetryStrategy

def _auto_fix_syntax_errors(self, dsl_code: str, errors: List[ValidationError]) -> Tuple[str, int]:
    """è‡ªåŠ¨ä¿®å¤è¯­æ³•é”™è¯¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    return EnhancedDSLAutoFixer.fix(dsl_code, errors)

def _should_retry_with_llm(self, result: ValidationResult) -> bool:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦ LLM é‡è¯•"""
    should_retry, reason = SmartRetryStrategy.should_retry_with_llm(result)
    debug(f"[é‡è¯•ç­–ç•¥] {reason}")
    return should_retry
```

**æ­¥éª¤4.3ï¼šæ›´æ–°å†å²è®°å½•æ•°æ®æ¨¡å‹**

```python
@dataclass
class PipelineHistory:
    # ... ç°æœ‰å­—æ®µ ...

    # è‡ªåŠ¨ä¿®å¤ç»Ÿè®¡
    auto_fix_stats: Dict[str, Any] = field(default_factory=dict)
    # {
    #   "total_fixes": 5,  # æ€»ä¿®å¤æ¬¡æ•°
    #   "syntax_errors_fixed": 3,  # è¯­æ³•é”™è¯¯ä¿®å¤
    #   "undefined_vars_fixed": 2,  # æœªå®šä¹‰å˜é‡ä¿®å¤
    #   "fix_success_rate": 0.8  # ä¿®å¤æˆåŠŸç‡
    # }
```

**æ­¥éª¤4.4ï¼šåœ¨ `view_history.py` ä¸­æ·»åŠ å¯è§†åŒ–æ”¯æŒ**

```python
def show_auto_fix_stats(pipeline_id: str):
    """æ˜¾ç¤ºè‡ªåŠ¨ä¿®å¤ç»Ÿè®¡"""
    manager = HistoryManager()
    history = manager.load_pipeline_history(pipeline_id)

    if not history:
        return

    info(f"\n{'='*90}")
    info(f"è‡ªåŠ¨ä¿®å¤ç»Ÿè®¡ - æµæ°´çº¿ {history.pipeline_id}")
    info(f"{'='*90}")

    if history.auto_fix_stats:
        stats = history.auto_fix_stats
        info(f"  æ€»ä¿®å¤æ¬¡æ•°: {stats.get('total_fixes', 0)}")
        info(f"  è¯­æ³•é”™è¯¯ä¿®å¤: {stats.get('syntax_errors_fixed', 0)}")
        info(f"  æœªå®šä¹‰å˜é‡ä¿®å¤: {stats.get('undefined_vars_fixed', 0)}")
        info(f"  æ§åˆ¶æµä¿®å¤: {stats.get('control_flow_fixed', 0)}")
        info(f"  ä¿®å¤æˆåŠŸç‡: {stats.get('fix_success_rate', 0)*100:.1f}%")
```

**é¢„æœŸæ•ˆæœï¼š**
- âœ… LLM é‡è¯•ï¼šå‡å°‘ 50-70%
- âœ… è‡ªåŠ¨ä¿®å¤æˆåŠŸç‡ï¼šæå‡åˆ° 70-80%
- âœ… å¯è§†åŒ–ï¼šå±•ç¤ºå„ç±»ä¿®å¤çš„ç»Ÿè®¡

---

## ğŸ“Š æŒä¹…åŒ–æ•°æ®æ±‡æ€»

### history_manager.py éœ€è¦æ·»åŠ çš„å­—æ®µ

```python
@dataclass
class ProcessingHistory:
    # ... ç°æœ‰å­—æ®µ ...
    rule_engine_stats: Dict[str, Any] = field(default_factory=dict)
    cache_hit: bool = False

@dataclass
class Prompt20History:
    # ... ç°æœ‰å­—æ®µ ...
    optimization_stats: Dict[str, Any] = field(default_factory=dict)
    cache_hits: int = 0
    cache_misses: int = 0

@dataclass
class PipelineHistory:
    # ... ç°æœ‰å­—æ®µ ...
    prompt10_rule_stats: Dict[str, Any] = field(default_factory=dict)
    prompt20_optimization_stats: Dict[str, Any] = field(default_factory=dict)
    prompt30_optimization_stats: Dict[str, Any] = field(default_factory=dict)
    total_cache_hits: int = 0
    total_cache_misses: int = 0
    cache_hit_rate: float = 0.0
    validation_stats: Dict[str, Any] = field(default_factory=dict)
    auto_fix_stats: Dict[str, Any] = field(default_factory=dict)
```

### view_history.py éœ€è¦æ·»åŠ çš„å‘½ä»¤

```python
# æ–°å¢å‘½ä»¤ï¼š
# python view_history.py metrics <pipeline_id>           # æ˜¾ç¤ºæ‰€æœ‰ä¼˜åŒ–æŒ‡æ ‡
# python view_history.py cache-stats <pipeline_id>       # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
# python view_history.py validation <pipeline_id>        # æ˜¾ç¤ºéªŒè¯è¯¦æƒ…
# python view_history.py auto-fix <pipeline_id>          # æ˜¾ç¤ºè‡ªåŠ¨ä¿®å¤ç»Ÿè®¡
# python view_history.py compare <pipeline_id1> <pipeline_id2>  # å¯¹æ¯”ä¼˜åŒ–æ•ˆæœ
```

---

## ğŸ¯ å®æ–½æ—¶é—´è¡¨

### ç¬¬1å‘¨ï¼šP0é˜¶æ®µ
- Day 1-3: ä¼˜åŒ–ç‚¹1ï¼ˆPrompt 1.0 è§„åˆ™åŒ–ï¼‰
- Day 4-7: ä¼˜åŒ–ç‚¹2ï¼ˆå®ä½“æå–ä¼˜åŒ–ï¼‰

### ç¬¬2å‘¨ï¼šP0é˜¶æ®µ + P1é˜¶æ®µ
- Day 8-9: ä¼˜åŒ–ç‚¹5ï¼ˆç¼“å­˜æœºåˆ¶ï¼‰
- Day 10-14: ä¼˜åŒ–ç‚¹3ï¼ˆDSL è½¬è¯‘ä¼˜åŒ–ï¼‰

### ç¬¬3å‘¨ï¼šP1é˜¶æ®µ
- Day 15-17: ä¼˜åŒ–ç‚¹6ï¼ˆä»£ç å±‚éªŒè¯è¦†ç›–åº¦ï¼‰
- Day 18-21: æµ‹è¯•ã€è°ƒè¯•ã€æ–‡æ¡£

### ç¬¬4-5å‘¨ï¼šP2é˜¶æ®µï¼ˆå¯é€‰ï¼‰
- Day 22-28: ä¼˜åŒ–ç‚¹4ï¼ˆè‡ªåŠ¨ä¿®å¤å¢å¼ºï¼‰
- Day 29-35: å…¨é¢æµ‹è¯•ã€æ€§èƒ½ä¼˜åŒ–

---

## âœ… éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] P0ä¼˜åŒ–å…¨éƒ¨å®æ–½å®Œæˆ
- [ ] æ‰€æœ‰æ–°å¢å­—æ®µæ­£ç¡®æŒä¹…åŒ–
- [ ] view_history.py æ”¯æŒæ‰€æœ‰æ–°å¢å‘½ä»¤
- [ ] ç¼“å­˜å‘½ä¸­ç‡ > 30%ï¼ˆé‡å¤åœºæ™¯ï¼‰
- [ ] LLM è°ƒç”¨æ¬¡æ•°å‡å°‘ > 50%

### æ€§èƒ½éªŒæ”¶
- [ ] æ ‡å‡†éœ€æ±‚å¤„ç†é€Ÿåº¦æå‡ > 2å€
- [ ] Token æ¶ˆè€—é™ä½ > 60%
- [ ] è‡ªåŠ¨ä¿®å¤æˆåŠŸç‡ > 70%

### å¯è§†åŒ–éªŒæ”¶
- [ ] ä¼˜åŒ–æŒ‡æ ‡æ¸…æ™°å±•ç¤º
- [ ] ç¼“å­˜ç»Ÿè®¡å‡†ç¡®æ˜¾ç¤º
- [ ] å¯¹æ¯”åŠŸèƒ½æ­£å¸¸å·¥ä½œ

---

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **å‘åå…¼å®¹**ï¼šæ‰€æœ‰æ–°å¢å­—æ®µä½¿ç”¨ `field(default_factory=dict)` ç¡®ä¿å‘åå…¼å®¹
2. **æ•°æ®è¿ç§»**ï¼šæ—§çš„å†å²è®°å½•ä¸ä¼šæœ‰æ–°å¢å­—æ®µï¼Œéœ€è¦åœ¨ä»£ç ä¸­åšåˆ¤ç©ºå¤„ç†
3. **ç¼“å­˜æ¸…ç†**ï¼šç¼“å­˜æ–‡ä»¶å­˜å‚¨åœ¨ `.cache/` ç›®å½•ï¼Œå»ºè®®å®šæœŸæ¸…ç†
4. **A/Bæµ‹è¯•**ï¼šå»ºè®®ä¿ç•™åŸç‰ˆæœ¬çš„ä»£ç ï¼Œè¿›è¡ŒA/Bæµ‹è¯•å¯¹æ¯”
5. **ç›‘æ§å‘Šè­¦**ï¼šæ–°å¢ä¼˜åŒ–æŒ‡æ ‡åï¼Œéœ€è¦ç›‘æ§æ˜¯å¦å‡ºç°å¼‚å¸¸ä¸‹é™

---

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. âœ… ç«‹å³å¼€å§‹å®æ–½ä¼˜åŒ–ç‚¹1ï¼ˆæœ€ç®€å•ï¼Œæ•ˆæœæœ€æ˜æ˜¾ï¼‰
2. âœ… åˆ›å»ºæ‰€æœ‰æ–°æ¨¡å—çš„åŸºç¡€æ¡†æ¶
3. âœ… åœ¨ view_history.py ä¸­æ·»åŠ ç»Ÿè®¡å±•ç¤ºåŠŸèƒ½
4. âœ… é€æ­¥é›†æˆå„ä¸ªä¼˜åŒ–ç‚¹
5. âœ… å»ºç«‹A/Bæµ‹è¯•æ¡†æ¶
