"""
ä¼˜åŒ–é›†æˆæµ‹è¯•è„šæœ¬
éªŒè¯ P0 é˜¶æ®µçš„æ‰€æœ‰ä¼˜åŒ–ç‚¹æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import time
import json
from pipeline import PromptPipeline, process_prompt
from data_models import ProcessingMode
from logger import info, warning, error


def test_prompt10_optimization():
    """æµ‹è¯• Prompt 1.0 è§„åˆ™åŒ–ä¼˜åŒ–"""
    info("\n" + "=" * 80)
    info("æµ‹è¯• 1: Prompt 1.0 è§„åˆ™åŒ–ä¼˜åŒ–")
    info("=" * 80)

    test_cases = [
        "å¸®æˆ‘æä¸€ä¸ªRAGçš„åº”ç”¨",
        "é‚£ä¸ªï¼Œå¸®æˆ‘å¼„ä¸€ä¸‹å¤§æ¨¡å‹çš„é¡¹ç›®å§",
        "éœ€è¦5ä¸ªäººï¼Œå…¶ä¸­2ä¸ªJavaï¼Œ3ä¸ªPython",
    ]

    term_mapping = {
        "å¤§æ¨¡å‹": "å¤§å‹è¯­è¨€æ¨¡å‹(LLM)",
        "RAG": "æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)",
    }

    total_llm_calls = 0
    total_time = 0

    for i, test_input in enumerate(test_cases, 1):
        info(f"\n>>> æµ‹è¯•ç”¨ä¾‹ {i}: {test_input}")

        pipeline = PromptPipeline(
            mode=ProcessingMode.DICTIONARY,
            term_mapping=term_mapping,
            use_mock_llm=False,
            enable_cache=False  # æš‚æ—¶ç¦ç”¨ç¼“å­˜ï¼Œæµ‹è¯•è§„åˆ™å¼•æ“
        )

        start = time.time()
        result = pipeline.run(test_input, stop_on_ambiguity=False)
        elapsed = (time.time() - start) * 1000
        total_time += elapsed

        # è·å– LLM è°ƒç”¨æ¬¡æ•°
        llm_calls = result.prompt10_result.llm_calls_count if result.prompt10_result else 0
        total_llm_calls += llm_calls

        # è·å–è§„åˆ™å¼•æ“ç»Ÿè®¡
        rule_stats = result.prompt10_result.rule_engine_stats if result.prompt10_result and hasattr(result.prompt10_result, 'rule_engine_stats') else {}

        info(f"  å¤„ç†æ—¶é—´: {elapsed:.2f}ms")
        info(f"  LLM è°ƒç”¨æ¬¡æ•°: {llm_calls}")
        info(f"  è§„åˆ™å¼•æ“å˜æ›´: {rule_stats.get('normalization_changes', 0)} å¤„")
        info(f"  å¤„ç†æ¨¡å¼: {rule_stats.get('processing_mode', 'unknown')}")

    avg_time = total_time / len(test_cases)
    info(f"\n>>> æµ‹è¯•ç»“æœæ€»ç»“:")
    info(f"  å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f}ms")
    info(f"  æ€» LLM è°ƒç”¨æ¬¡æ•°: {total_llm_calls}")
    info(f"  æ¯ä¸ª case å¹³å‡ LLM è°ƒç”¨: {total_llm_calls / len(test_cases):.2f}")

    # éªŒè¯ä¼˜åŒ–æ•ˆæœ
    if total_llm_calls == 0:
        info("âœ… Prompt 1.0 ä¼˜åŒ–é€šè¿‡: LLM è°ƒç”¨ä¸º 0ï¼Œè§„åˆ™å¼•æ“å®Œå…¨æ¥ç®¡")
        return True
    else:
        warning(f"âš ï¸ Prompt 1.0 ä¼˜åŒ–æœªå®Œå…¨ç”Ÿæ•ˆ: ä»æœ‰ {total_llm_calls} æ¬¡ LLM è°ƒç”¨")
        return False


def test_prompt20_optimization():
    """æµ‹è¯• Prompt 2.0 å®ä½“æå–ä¼˜åŒ–"""
    info("\n" + "=" * 80)
    info("æµ‹è¯• 2: Prompt 2.0 å®ä½“æå–ä¼˜åŒ–")
    info("=" * 80)

    test_cases = [
        "é¡¹ç›®éœ€è¦5ä¸ªäººï¼Œä¸ºæœŸ2å‘¨",
        "é¢„ç®—50ä¸‡ï¼Œéœ€è¦3ä¸ªJavaå¼€å‘äººå‘˜",
        "å¤„ç†æ—¶é—´é™åˆ¶åœ¨2ç§’ä»¥å†…ï¼Œæ”¯æŒä¸­è‹±æ–‡åŒè¯­",
    ]

    total_llm_calls = 0
    total_time = 0
    total_regex_entities = 0

    for i, test_input in enumerate(test_cases, 1):
        info(f"\n>>> æµ‹è¯•ç”¨ä¾‹ {i}: {test_input}")

        pipeline = PromptPipeline(
            mode=ProcessingMode.DICTIONARY,
            use_mock_llm=False,
            enable_cache=False  # æš‚æ—¶ç¦ç”¨ç¼“å­˜
        )

        start = time.time()
        result = pipeline.run(test_input, stop_on_ambiguity=False)
        elapsed = (time.time() - start) * 1000
        total_time += elapsed

        # è·å–ä¼˜åŒ–ç»Ÿè®¡
        opt_stats = {}
        if hasattr(pipeline, 'llm_client'):
            # ä» llm_client è·å–ç»Ÿè®¡ï¼ˆéœ€è¦åœ¨å®é™…è°ƒç”¨ä¸­å®ç°ï¼‰
            opt_stats = getattr(pipeline.llm_client, '_last_optimization_stats', {})

        info(f"  å¤„ç†æ—¶é—´: {elapsed:.2f}ms")
        info(f"  æå–å˜é‡æ•°: {len(result.prompt20_result.variables) if result.prompt20_result else 0}")

    avg_time = total_time / len(test_cases)
    info(f"\n>>> æµ‹è¯•ç»“æœæ€»ç»“:")
    info(f"  å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f}ms")

    # éªŒè¯ä¼˜åŒ–æ•ˆæœ
    if avg_time < 100:  # å¦‚æœå¹³å‡æ—¶é—´å°äº 100msï¼Œè¯´æ˜ä¼˜åŒ–æœ‰æ•ˆ
        info("âœ… Prompt 2.0 ä¼˜åŒ–é€šè¿‡: å¤„ç†é€Ÿåº¦æ˜¾è‘—æå‡")
        return True
    else:
        warning(f"âš ï¸ Prompt 2.0 ä¼˜åŒ–æ•ˆæœæœ‰é™: å¹³å‡å¤„ç†æ—¶é—´ {avg_time:.2f}ms")
        return False


def test_cache_optimization():
    """æµ‹è¯•ç¼“å­˜æœºåˆ¶ä¼˜åŒ–"""
    info("\n" + "=" * 80)
    info("æµ‹è¯• 3: ç¼“å­˜æœºåˆ¶ä¼˜åŒ–")
    info("=" * 80)

    test_input = "å¸®æˆ‘æä¸€ä¸ªRAGçš„åº”ç”¨"

    # ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
    info("\n>>> ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰:")
    pipeline1 = PromptPipeline(
        mode=ProcessingMode.DICTIONARY,
        use_mock_llm=False,
        enable_cache=True  # å¯ç”¨ç¼“å­˜
    )

    start1 = time.time()
    result1 = pipeline1.run(test_input)
    time1 = (time.time() - start1) * 1000

    # è·å–ç¼“å­˜ç»Ÿè®¡
    cache_stats1 = pipeline1.llm_client.get_cache_stats()

    info(f"  å¤„ç†æ—¶é—´: {time1:.2f}ms")
    info(f"  ç¼“å­˜å‘½ä¸­: {cache_stats1.get('hits', 0)}")
    info(f"  ç¼“å­˜æœªå‘½ä¸­: {cache_stats1.get('misses', 0)}")

    # ç¬¬äºŒæ¬¡è¿è¡Œï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
    info("\n>>> ç¬¬äºŒæ¬¡è¿è¡Œï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰:")
    pipeline2 = PromptPipeline(
        mode=ProcessingMode.DICTIONARY,
        use_mock_llm=False,
        enable_cache=True
    )

    start2 = time.time()
    result2 = pipeline2.run(test_input)
    time2 = (time.time() - start2) * 1000

    # è·å–ç¼“å­˜ç»Ÿè®¡
    cache_stats2 = pipeline2.llm_client.get_cache_stats()

    info(f"  å¤„ç†æ—¶é—´: {time2:.2f}ms")
    info(f"  ç¼“å­˜å‘½ä¸­: {cache_stats2.get('hits', 0)}")
    info(f"  ç¼“å­˜æœªå‘½ä¸­: {cache_stats2.get('misses', 0)}")

    # è®¡ç®—åŠ é€Ÿæ¯”
    if time2 > 0:
        speedup = time1 / time2
        info(f"\n>>> ç¼“å­˜åŠ é€Ÿæ•ˆæœ:")
        info(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")

        if speedup > 2:
            info("âœ… ç¼“å­˜ä¼˜åŒ–é€šè¿‡: åŠ é€Ÿæ•ˆæœæ˜¾è‘—")
            return True
        else:
            warning(f"âš ï¸ ç¼“å­˜ä¼˜åŒ–æ•ˆæœæœ‰é™: åŠ é€Ÿæ¯” {speedup:.2f}x")
            return False
    else:
        warning("âš ï¸ æ— æ³•è®¡ç®—åŠ é€Ÿæ¯”")
        return False


def test_overall_optimization():
    """æµ‹è¯•æ•´ä½“ä¼˜åŒ–æ•ˆæœ"""
    info("\n" + "=" * 80)
    info("æµ‹è¯• 4: æ•´ä½“ä¼˜åŒ–æ•ˆæœ")
    info("=" * 80)

    test_input = "å¸®æˆ‘è®¾è®¡ä¸€ä¸ª5äººçš„å›¢é˜Ÿï¼Œå¼€å‘åŸºäºRAGçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œé¢„è®¡éœ€è¦2ä¸ªæœˆ"

    # åˆ›å»ºæµæ°´çº¿ï¼ˆå¯ç”¨æ‰€æœ‰ä¼˜åŒ–ï¼‰
    pipeline = PromptPipeline(
        mode=ProcessingMode.DICTIONARY,
        term_mapping={
            "å¤§æ¨¡å‹": "å¤§å‹è¯­è¨€æ¨¡å‹(LLM)",
            "RAG": "æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)",
        },
        use_mock_llm=False,
        enable_cache=True
    )

    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    start = time.time()
    result = pipeline.run(test_input)
    total_time = (time.time() - start) * 1000

    # æ”¶é›†ä¼˜åŒ–ç»Ÿè®¡
    llm_calls = result.prompt10_result.llm_calls_count if result.prompt10_result else 0
    rule_stats = result.prompt10_result.rule_engine_stats if result.prompt10_result and hasattr(result.prompt10_result, 'rule_engine_stats') else {}
    cache_stats = pipeline.llm_client.get_cache_stats()

    info(f"\n>>> æ•´ä½“ä¼˜åŒ–ç»Ÿè®¡:")
    info(f"  æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ms")
    info(f"  LLM è°ƒç”¨æ¬¡æ•°: {llm_calls}")
    info(f"  è§„åˆ™å¼•æ“å˜æ›´: {rule_stats.get('normalization_changes', 0)} å¤„")
    info(f"  ç¼“å­˜å‘½ä¸­ç‡: {cache_stats.get('hit_rate', 0) * 100:.2f}%")
    info(f"  æå–å˜é‡æ•°: {len(result.prompt20_result.variables) if result.prompt20_result else 0}")

    # è¯„ä¼°æ•´ä½“æ•ˆæœ
    if llm_calls == 0 and total_time < 3000:
        info("âœ… æ•´ä½“ä¼˜åŒ–é€šè¿‡: LLM è°ƒç”¨ä¸º 0ï¼Œå¤„ç†é€Ÿåº¦æ»¡è¶³è¦æ±‚")
        return True
    else:
        warning(f"âš ï¸ æ•´ä½“ä¼˜åŒ–æœ‰å¾…æ”¹è¿›: LLM è°ƒç”¨ {llm_calls} æ¬¡ï¼Œå¤„ç†æ—¶é—´ {total_time:.2f}ms")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    info("\n" + "â–ˆ" * 80)
    info("â–ˆ" + " " * 25 + "ä¼˜åŒ–é›†æˆæµ‹è¯•" + " " * 39 + "â–ˆ")
    info("â–ˆ" * 80)

    results = {
        "Prompt 1.0 è§„åˆ™åŒ–": None,
        "Prompt 2.0 å®ä½“æå–": None,
        "ç¼“å­˜æœºåˆ¶": None,
        "æ•´ä½“ä¼˜åŒ–æ•ˆæœ": None
    }

    try:
        results["Prompt 1.0 è§„åˆ™åŒ–"] = test_prompt10_optimization()
    except Exception as e:
        error(f"Prompt 1.0 æµ‹è¯•å¤±è´¥: {e}")
        results["Prompt 1.0 è§„åˆ™åŒ–"] = False

    try:
        results["Prompt 2.0 å®ä½“æå–"] = test_prompt20_optimization()
    except Exception as e:
        error(f"Prompt 2.0 æµ‹è¯•å¤±è´¥: {e}")
        results["Prompt 2.0 å®ä½“æå–"] = False

    try:
        results["ç¼“å­˜æœºåˆ¶"] = test_cache_optimization()
    except Exception as e:
        error(f"ç¼“å­˜æœºåˆ¶æµ‹è¯•å¤±è´¥: {e}")
        results["ç¼“å­˜æœºåˆ¶"] = False

    try:
        results["æ•´ä½“ä¼˜åŒ–æ•ˆæœ"] = test_overall_optimization()
    except Exception as e:
        error(f"æ•´ä½“ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
        results["æ•´ä½“ä¼˜åŒ–æ•ˆæœ"] = False

    # æ€»ç»“æŠ¥å‘Š
    info("\n" + "â–ˆ" * 80)
    info("â–ˆ" + " " * 30 + "æµ‹è¯•æ€»ç»“" + " " * 36 + "â–ˆ")
    info("â–ˆ" * 80)

    passed = sum(1 for v in results.values() if v)
    total = len(results)

    info(f"\n>>> æµ‹è¯•ç»“æœ:")
    for test_name, passed_flag in results.items():
        status = "âœ… é€šè¿‡" if passed_flag else "âŒ å¤±è´¥"
        info(f"  {test_name}: {status}")

    info(f"\n>>> æ€»ä½“è¯„ä¼°:")
    info(f"  é€šè¿‡: {passed}/{total}")
    if passed == total:
        info("  ğŸ‰ æ‰€æœ‰ä¼˜åŒ–ç‚¹æµ‹è¯•é€šè¿‡ï¼")
    elif passed >= total * 0.75:
        info("  âš ï¸ å¤§éƒ¨åˆ†ä¼˜åŒ–ç‚¹æµ‹è¯•é€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        info("  âŒ å¤šæ•°ä¼˜åŒ–ç‚¹æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›")

    # å¯¼å‡ºæµ‹è¯•æŠ¥å‘Š
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "results": results,
        "summary": {
            "total": total,
            "passed": passed,
            "pass_rate": passed / total if total > 0 else 0
        }
    }

    report_file = "optimization_test_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    info(f"\n>>> æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    return passed == total


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
